{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk; #nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lara\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lara\\Anaconda3\\lib\\site-packages\\thinc\\neural\\train.py:3: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from .optimizers import Adam, SGD, linear_decay\n",
      "C:\\Users\\lara\\Anaconda3\\lib\\site-packages\\thinc\\check.py:1: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import defaultdict, Sequence, Sized, Iterable, Callable\n"
     ]
    }
   ],
   "source": [
    "# spacy for lemmatization\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index('train_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.item_description.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This keyboard is in great condition and works like it came out of the box. All of the ports are tested and work perfectly. The lights are customizable via the Razer Synapse app on your PC.', 'Adorable top with a hint of lace and a key hole in the back! The pale pink is a 1X, and I also have a 3X available in white!', 'New with tags. Leather horses. Retail for [rm] each. Stand about a foot high. They are being sold as a pair. Any questions please ask. Free shipping. Just got out of storage', 'Complete with certificate of authenticity', 'Banana republic bottoms, Candies skirt with matching blazer,Amy Byers suit, Loft bottoms and cami top.', 'Size small but straps slightly shortened to fit xs, besides that, perfect condition', 'You get three pairs of Sophie cheer shorts size small and medium girls and two sports bra/boy shorts spandex matching sets in small and medium girls. All items total retail for [rm] in store and you can take him today for less than the price of one item at the store!)', 'Girls Size small Plus green. Three shorts total.', 'I realized his pants are on backwards after the picture. They were very dirty so I hand washed them. He has a stuffed body and painted porcelain head, hands and feet. Back before clowns were too scary. 9\" tall. No chips or cracks but minor paint loss in a few places. Clown Circus Doll Collectible']\n"
     ]
    }
   ],
   "source": [
    "print(data2[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This keyboard is in great condition and works like it came out of the box. All of the ports are tested and work perfectly. The lights are customizable via the Razer Synapse app on your PC.', 'Adorable top with a hint of lace and a key hole in the back! The pale pink is a 1X, and I also have a 3X available in white!', 'New with tags. Leather horses. Retail for  each. Stand about a foot high. They are being sold as a pair. Any questions please ask. Free shipping. Just got out of storage', 'Complete with certificate of authenticity', 'Banana republic bottoms, Candies skirt with matching blazer,Amy Byers suit, Loft bottoms and cami top.', 'Size small but straps slightly shortened to fit xs, besides that, perfect condition', 'You get three pairs of Sophie cheer shorts size small and medium girls and two sports bra/boy shorts spandex matching sets in small and medium girls. All items total retail for  in store and you can take him today for less than the price of one item at the store!)', 'Girls Size small Plus green. Three shorts total.', 'I realized his pants are on backwards after the picture. They were very dirty so I hand washed them. He has a stuffed body and painted porcelain head, hands and feet. Back before clowns were too scary. 9\" tall. No chips or cracks but minor paint loss in a few places. Clown Circus Doll Collectible']\n"
     ]
    }
   ],
   "source": [
    "data2 = [re.sub('\\[rm\\]', '', str(sent)) for sent in data2]\n",
    "print(data2[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'keyboard', 'is', 'in', 'great', 'condition', 'and', 'works', 'like', 'it', 'came', 'out', 'of', 'the', 'box', 'all', 'of', 'the', 'ports', 'are', 'tested', 'and', 'work', 'perfectly', 'the', 'lights', 'are', 'customizable', 'via', 'the', 'razer', 'synapse', 'app', 'on', 'your', 'pc'], ['adorable', 'top', 'with', 'hint', 'of', 'lace', 'and', 'key', 'hole', 'in', 'the', 'back', 'the', 'pale', 'pink', 'is', 'and', 'also', 'have', 'available', 'in', 'white'], ['new', 'with', 'tags', 'leather', 'horses', 'retail', 'for', 'each', 'stand', 'about', 'foot', 'high', 'they', 'are', 'being', 'sold', 'as', 'pair', 'any', 'questions', 'please', 'ask', 'free', 'shipping', 'just', 'got', 'out', 'of', 'storage'], ['complete', 'with', 'certificate', 'of', 'authenticity'], ['banana', 'republic', 'bottoms', 'candies', 'skirt', 'with', 'matching', 'blazer', 'amy', 'byers', 'suit', 'loft', 'bottoms', 'and', 'cami', 'top'], ['size', 'small', 'but', 'straps', 'slightly', 'shortened', 'to', 'fit', 'xs', 'besides', 'that', 'perfect', 'condition'], ['you', 'get', 'three', 'pairs', 'of', 'sophie', 'cheer', 'shorts', 'size', 'small', 'and', 'medium', 'girls', 'and', 'two', 'sports', 'bra', 'boy', 'shorts', 'spandex', 'matching', 'sets', 'in', 'small', 'and', 'medium', 'girls', 'all', 'items', 'total', 'retail', 'for', 'in', 'store', 'and', 'you', 'can', 'take', 'him', 'today', 'for', 'less', 'than', 'the', 'price', 'of', 'one', 'item', 'at', 'the', 'store'], ['girls', 'size', 'small', 'plus', 'green', 'three', 'shorts', 'total'], ['realized', 'his', 'pants', 'are', 'on', 'backwards', 'after', 'the', 'picture', 'they', 'were', 'very', 'dirty', 'so', 'hand', 'washed', 'them', 'he', 'has', 'stuffed', 'body', 'and', 'painted', 'porcelain', 'head', 'hands', 'and', 'feet', 'back', 'before', 'clowns', 'were', 'too', 'scary', 'tall', 'no', 'chips', 'or', 'cracks', 'but', 'minor', 'paint', 'loss', 'in', 'few', 'places', 'clown', 'circus', 'doll', 'collectible']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data2))\n",
    "\n",
    "print(data_words[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1482535"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lara\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words_nostops) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words_nostops])  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lightly', 'worn', 'signs_wear', 'black', 'white', 'knee_length', 'sweater', 'could_fit', 'small', 'medium', 'possibly', 'even', 'large', 'purchased', 'karmaloop', 'clothes_accessories', 'similar', 'nordstrom', 'american_apparel', 'true_religion', 'free_people_urban', 'outfitters_zara', 'asos_topshop', 'lulus_tobi', 'american_apparel_brandy_melville', 'unif_nasty', 'gal_lf', 'forever', 'rebecca_minkoff', 'sephora', 'kate_spade', 'michael_kors', 'vince_camuto', 'victoria_secret_pink', 'vs', 'etc']\n"
     ]
    }
   ],
   "source": [
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words_nostops[1685]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_trigrams = make_trigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy  download en\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                            alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.014*\"back\" + 0.013*\"case\" + 0.010*\"front\" + 0.010*\"look\" + 0.008*\"design\" '\n",
      "  '+ 0.008*\"piece\" + 0.008*\"super_cute\" + 0.007*\"love\" + 0.007*\"pocket\" + '\n",
      "  '0.006*\"long\"'),\n",
      " (1,\n",
      "  '0.024*\"item\" + 0.020*\"bundle\" + 0.017*\"ship\" + 0.015*\"shipping\" + '\n",
      "  '0.015*\"free\" + 0.013*\"include\" + 0.013*\"set\" + 0.013*\"make\" + 0.012*\"get\" + '\n",
      "  '0.010*\"purchase\"'),\n",
      " (2,\n",
      "  '0.124*\"new\" + 0.067*\"brand\" + 0.026*\"use\" + 0.022*\"come\" + 0.022*\"box\" + '\n",
      "  '0.019*\"color\" + 0.015*\"price_firm\" + 0.014*\"authentic\" + 0.012*\"free_shipp\" '\n",
      "  '+ 0.012*\"good\"'),\n",
      " (3,\n",
      "  '0.087*\"size\" + 0.028*\"condition\" + 0.027*\"black\" + 0.025*\"great\" + '\n",
      "  '0.020*\"small\" + 0.016*\"fit\" + 0.015*\"medium\" + 0.014*\"pink\" + 0.014*\"white\" '\n",
      "  '+ 0.014*\"wear\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -8.607253928342942\n",
      "\n",
      "Coherence Score:  0.5125941023795811\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word) \n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data_lemmatized', 'wb') as f:\n",
    "    pickle.dump(data_lemmatized, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dostaaaa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "# Save model to disk.\n",
    "temp_file = datapath(\"model\")\n",
    "lda_model.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('doc_lda.pickle', 'wb') as f:\n",
    "    pickle.dump(doc_lda , f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
